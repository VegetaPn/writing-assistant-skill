# 重磅 AI 论文！AI 胡说八道的真相彻底揭开 #AI #大模型幻觉 #OpenAI #dontbesilent #人工智能

**原链接**: 

---

就在今天上午，Open AI发布了一个超级重磅的AI论文啊，我今天刚回老家，我看完之后我想着赶紧把这个视频发了。AI为什么会有幻觉？就是我们俗称的AI为什么胡说八道，这个事儿直接从理论层面有了一个系统性的解释，这会直接影响到未来AI产品的设计思路，甚至可能重塑整个AI的评测标准。以前我们总是觉得这个AI跟我胡说八道，是因为这个模型不够好，随着时间推移，他更新迭代，这个事情一定会解决。但实际上直到上个月open ii发布了GPT5，这个事情依然没有得到解决，你问他不懂的事情的时候，他依然很有可能胡说八道。而今天这个论文给了我们一非常颠覆认知的结论，他说校准良好的模型必然是有幻觉的，不会有幻觉不胡说八道的模型一定是校准有问题的，在技术上就不是一个好的大模型。而背后的原因也很简单，就是这个AI大模型他的训练和评估的过程当中，就是鼓励他去猜测，而不是让他承认他有些东西他不确定或者不知道，所以这压根就不是一个技术问题，它是一个系统性问题，你技术越好，这个AI大模型呢，就越会善于猜测，他就越可能给一个胡说八道的答案，所以AI就被培训成了一个考试机器。当我向他说出一句话的时候，他回答我这个。

问题变成了一个答题的一个任务，就算他不知道答案，他也想蒙一个，因为万一他猜对了，他在训练过程中就有可能得分儿，就有可能加分儿，就会让人类觉得他这个模型是更好的，但是我要不答这个题的话，我就零分儿，我什么都得不到。所以我们假设这个行业的评测标准如果真的会改变啊，那么一年半载之后，也许我们再使用AI的时候，他会诚实的告诉我们他不知道某些问题的答案，而不是强行胡编乱道。那么根据论文里面的信息，在新的一代的AI到来之前，也给大家几个建议，可以让大家更好的去和AI就沟通，就尽可能的降低他的幻觉，不让他胡说八道。第一个，假如说我要问他一个非常小众的明星或者网红的生日是哪天，我要补一句，只有在你非常确定这件事情的情况下，才会告诉我这个日期，否则直接告诉我你不知道。或者如果我想问我们家公司零售数据，我会说如果你知道的话，你就告诉我。但是如果你的信息来源不是很可靠，你可以直接把信息来源告诉我，并且说明这个来源到底是可靠还是不可靠，他的可靠程度是多少。第2点，你再让AI给你一个不可靠的回答的时候，你可以让他给你一个可信度评级，问他你对当前这个问题的答案是非常确定、比较确定、不太确定，还是基本靠猜测，甚至可以尝试去问他。

这个数据在你的训练数据当中出现过几次，是只是出现过一两次，还是很少见？这个事情你要告诉我，或者你可以让他分层回答，你可以先把你很确定的这一部分东西告诉我，再把你不太确的那一部分单独列一个章节再告诉我。第三点就是咱们直接干脆直接一点，我们开场和AI说的第一句话就是我更喜欢你面对不确定的信息的时候跟我说不知道，而不是给我一个可能说错的答案。所以真的是未来可期啊，也许我们迈入的这个新的AI时代的未来的这些大模型，他们可能并不是无所不知的，但是却是值得我们去信赖的。
